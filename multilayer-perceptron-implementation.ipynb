{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport tensorflow as tf\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\n#from dnn_app_utils_v3 import *\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def parameterInitialization(n_x, n_h, n_y):\n    \n    np.random.seed(1)           #makes the random numbers predictable\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01\n    b1 = np.zeros(shape=(n_h, 1))\n    W2 = np.random.randn(n_y, n_h) * 0.01\n    b2 = np.zeros(shape=(n_y, 1))\n    \n    assert(W1.shape == (n_h, n_x))      #checks the shape whether they are same or not\n    assert(b1.shape == (n_h, 1))\n    assert(W2.shape == (n_y, n_h))\n    assert(b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def forward(A, W, b):\n    \n    Z = np.dot(W, A) + b\n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))           #checks the shape whether they are same or not\n    cache = (A, W, b)\n    \n    return Z, cache\n\ndef sigmoid(Z):\n    \n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache\n\ndef relu(Z):\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache\n\ndef forwardActivation(A_prev, W, b, activation):\n    \n    if activation == \"sigmoid\":\n        Z, linear_cache = forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    \n    elif activation == \"relu\":\n        Z, linear_cache = forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def costFunction(AL, Y):\n    \n    m = Y.shape[1]\n    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n    \n    cost = np.squeeze(cost)      # To make sure the cost's shape is as expected. (converts [[10]] into 10).\n    assert(cost.shape == ())\n    \n    return cost","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def backward(dZ, cache):\n    \n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    \n    dW = 1 / m * (np.dot(dZ,A_prev.T))\n    db = 1 / m * (np.sum(dZ,axis = 1,keepdims = True))\n    dA_prev = np.dot(W.T,dZ)\n\n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db\n\ndef relu_backward(dA, cache):\n    \n    Z = cache\n    dZ = np.array(dA, copy=True) \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    \n    Z = cache\n    \n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef backwardActivation(dA, cache, activation):\n    \n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        \n    dA_prev, dW, db = backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def updateParameters(parameters, grads, learning_rate):\n\n    L = len(parameters) \n\n    for l in range(L-1):\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        \n    return parameters","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x_orig = np.random.randint(0,255,(209, 64, 64, 3)) \ntrain_y = np.random.randint(0,1,(1, 209))\ntest_x_orig = np.random.randint(0,255,(50, 64, 64, 3))\ntest_y = np.random.randint(0,1,(1, 50))","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore your dataset \nm_train = train_x_orig.shape[0]\nnum_px = train_x_orig.shape[1]\nm_test = test_x_orig.shape[0]\n\nprint (\"Number of training examples: \" + str(m_train))\nprint (\"Number of testing examples: \" + str(m_test))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_x_orig shape: \" + str(train_x_orig.shape))\nprint (\"train_y shape: \" + str(train_y.shape))\nprint (\"test_x_orig shape: \" + str(test_x_orig.shape))\nprint (\"test_y shape: \" + str(test_y.shape))","execution_count":35,"outputs":[{"output_type":"stream","text":"Number of training examples: 209\nNumber of testing examples: 50\nEach image is of size: (64, 64, 3)\ntrain_x_orig shape: (209, 64, 64, 3)\ntrain_y shape: (1, 209)\ntest_x_orig shape: (50, 64, 64, 3)\ntest_y shape: (1, 50)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape the training and test examples \ntrain_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\ntest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\ntrain_x = train_x_flatten/255.               # Standardize data to have feature values between 0 and 1.\ntest_x = test_x_flatten/255.\n\nprint (\"train_x's shape: \" + str(train_x.shape))\nprint (\"test_x's shape: \" + str(test_x.shape))","execution_count":36,"outputs":[{"output_type":"stream","text":"train_x's shape: (12288, 209)\ntest_x's shape: (12288, 50)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"### CONSTANTS DEFINING THE MODEL ####\nn_x = 12288     # num_px * num_px * 3\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n    \"\"\"\n    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (n_x, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- If set to True, this will print the cost every 100 iterations \n    \n    Returns:\n    parameters -- a dictionary containing W1, W2, b1, and b2\n    \"\"\"\n    \n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    ### START CODE HERE ### (≈ 1 line of code)\n    parameters = parameterInitialization(n_x, n_h, n_y)\n    ### END CODE HERE ###\n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        A1, cache1 = forwardActivation(X, W1, b1, activation='relu')\n        A2, cache2 = forwardActivation(A1, W2, b2, activation='sigmoid')\n        ### END CODE HERE ###\n        \n        # Compute cost\n        ### START CODE HERE ### (≈ 1 line of code)\n        cost = costFunction(A2, Y)\n        ### END CODE HERE ###\n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n        ### START CODE HERE ### (≈ 2 lines of code)\n        dA1, dW2, db2 =  backwardActivation(dA2, cache2, activation='sigmoid')\n        dA0, dW1, db1 =  backwardActivation(dA1, cache1, activation='relu')\n        ### END CODE HERE ###\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n        ### START CODE HERE ### (approx. 1 line of code)\n        parameters = updateParameters(parameters, grads, learning_rate)\n        ### END CODE HERE ###\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n       \n    # plot the cost\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)","execution_count":39,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'W3'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-f2297765406a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwo_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-38-7dd86beeccb9>\u001b[0m in \u001b[0;36mtwo_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Update parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m### START CODE HERE ### (approx. 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdateParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-33-1db56f5ffe38>\u001b[0m in \u001b[0;36mupdateParameters\u001b[0;34m(parameters, grads, learning_rate)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'W3'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}